{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import timm\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from datetime import datetime\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "SIZE=48\n",
    "BATCH_SIZE=512\n",
    "NUM_CLASSES=7"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class FERDataset(Dataset):\n",
    "    def __init__(self, directory, transform=None):\n",
    "        self.root_dir = directory\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "\n",
    "        self.label_dict = {\"angry\": 0, \"disgusted\": 1, \"fearful\": 2, \"happy\": 3, \"natural\": 4, \"sadness\": 5, \"surprised\": 6}\n",
    "\n",
    "        for label in os.listdir(directory):\n",
    "            label_path = os.path.join(directory, label)\n",
    "            if os.path.isdir(label_path):\n",
    "                for img_file in os.listdir(label_path):\n",
    "                    self.images.append(os.path.join(label_path, img_file))\n",
    "                    self.labels.append(self.label_dict[label])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.labels[idx]\n",
    "        img_path = self.images[idx]\n",
    "        image = Image.open(img_path)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((SIZE, SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = FERDataset(directory='../data/data_fer/train', transform=transform)\n",
    "test_dataset = FERDataset(directory='../data/data_fer/test', transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class EmotionEstimatorModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EmotionEstimatorModel, self).__init__()\n",
    "        # Загрузка модели EfficientNetV2\n",
    "        self.base_model = timm.create_model('efficientnetv2_rw_s', pretrained=True)\n",
    "        # Заменяем первый сверточный слой. Создаем новый сверточный слой с 1 входным каналом и тем же количеством выходных каналов\n",
    "        self.base_model.conv_stem = nn.Conv2d(in_channels=1, \n",
    "                                              out_channels=self.base_model.conv_stem.out_channels,\n",
    "                                              kernel_size=self.base_model.conv_stem.kernel_size, \n",
    "                                              stride=self.base_model.conv_stem.stride, \n",
    "                                              padding=self.base_model.conv_stem.padding, \n",
    "                                              bias=False)\n",
    "        # self.base_model.conv_stem.weight.data = self.base_model.conv_stem.weight.data.sum(dim=1, keepdim=True)\n",
    "        # Заменяем классификатор для соответствия числу классов\n",
    "        self.base_model.classifier = nn.Linear(self.base_model.classifier.in_features, NUM_CLASSES)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.base_model(x)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = EmotionEstimatorModel().to(device)\n",
    "loss_fun = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "writer = SummaryWriter(log_dir=\"log/emotion\", filename_suffix=datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "\n",
    "for epoch in tqdm(range(100)):  # проход по датасету несколько раз\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = loss_fun(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        writer.add_scalar('Metrics/epoch_loss', running_loss  / len(train_loader), epoch)\n",
    "    \n",
    "\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for i, (images, labels) in enumerate(test_loader):\n",
    "            images, labels = images.to(device), labels\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_preds.extend(predicted.tolist())\n",
    "            all_labels.extend(labels.tolist())\n",
    "    precision = precision_score(all_labels, all_preds, average=\"weighted\")\n",
    "    recall = recall_score(all_labels, all_preds, average=\"weighted\")\n",
    "    f1 = f1_score(all_labels, all_preds, average=\"weighted\")\n",
    "    writer.add_scalar('Metrics/precision', precision, epoch)\n",
    "    writer.add_scalar('Metrics/recall', recall, epoch)\n",
    "    writer.add_scalar('Metrics/f1', f1, epoch)\n",
    "print('Finished Training')"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Функция для вычисления предсказаний\n",
    "def get_predictions(model, loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_preds.extend(predicted.tolist())\n",
    "            all_labels.extend(labels.tolist())\n",
    "    return all_labels, all_preds\n",
    "\n",
    "# Получение предсказаний на валидационном наборе\n",
    "val_labels, val_preds = get_predictions(model, test_loader)\n",
    "\n",
    "# Вычисление метрик\n",
    "precision = precision_score(val_labels, val_preds, average=None)\n",
    "recall = recall_score(val_labels, val_preds, average=None)\n",
    "f1 = f1_score(val_labels, val_preds, average=None)\n",
    "\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "torch.save(model, '../../models/emotion_model_torch.pth')\n",
    "torch.save(model.state_dict(), '../../models/emotion_model_weights.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# model = AgeEstimatorModel()  # Создайте экземпляр вашей модели\n",
    "# model.load_state_dict(torch.load('../../emotion_model_torch.pth'))\n",
    "\n",
    "model = torch.load('../../models/emotion_model_torch.pth')\n",
    "\n",
    "model.eval()  # Переведите модель в режим оценки\n",
    "\n",
    "# Загрузите изображение\n",
    "image_path = '/home/vorkov/Workspace/EDA/learning/data/UTKFace_48/2_0_2_20161219141143184.jpg.chip.jpg'\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Примените преобразования к изображению\n",
    "image = transform(image)\n",
    "image = image.to(device)\n",
    "image = image.unsqueeze(0)  # Добавьте дополнительное измерение, так как модель ожидает пакет изображений\n",
    "\n",
    "# Сделайте предсказание\n",
    "with torch.no_grad():\n",
    "    output = model(image)\n",
    "    _, predicted = torch.max(output, 1)\n",
    "    predicted_age = predicted.item()  # Получите предсказанный возраст как число\n",
    "\n",
    "print(f'Predicted Emotion: {predicted_age}')"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
