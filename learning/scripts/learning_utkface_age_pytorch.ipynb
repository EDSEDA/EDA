{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-28 19:20:36.453096: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-28 19:20:36.475475: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-28 19:20:36.475508: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-28 19:20:36.475551: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-28 19:20:36.481071: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import timm\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from datetime import datetime\n",
    "\n",
    "from inference.models import AgeEstimatorModel"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-28T16:20:37.615282628Z",
     "start_time": "2024-01-28T16:20:34.784904884Z"
    }
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "SIZE=48\n",
    "BATCH_SIZE=200\n",
    "EPOCHS=80\n",
    "LR=0.0005\n",
    "\n",
    "DATASET_PATH=\"../data/UTKFace_48\"\n",
    "LOG_PATH=\"../../logs/age\"\n",
    "MODEL_PATH=\"../../models/age_model_torch.pth\"\n",
    "WEIGHTS_PATH=\"../../models/age_model_weights.pth\"\n",
    "TEST_IMAGE_PATH=\"../data/face_recognition_images/person1.1.jpg\"\n",
    "TIME_FORMAT=\"%d-%m-%Y; %H:%M:%S\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-28T16:20:37.659963099Z",
     "start_time": "2024-01-28T16:20:37.616259320Z"
    }
   },
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((SIZE, SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-28T16:20:37.660561564Z",
     "start_time": "2024-01-28T16:20:37.659801083Z"
    }
   },
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AgeEstimatorModel().to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-28T16:20:38.677621912Z",
     "start_time": "2024-01-28T16:20:37.659937639Z"
    }
   },
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class UTKFaceDataset(Dataset):\n",
    "    def __init__(self, directory, transform=None):\n",
    "        self.files = glob.glob(os.path.join(directory, '*.jpg'))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.files[idx]\n",
    "        image = Image.open(img_name)\n",
    "        filename = img_name.split('/')[-1]\n",
    "        age = int(filename.split('_')[0])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, age"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-28T12:26:07.556915460Z",
     "start_time": "2024-01-28T12:26:07.556040949Z"
    }
   },
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dataset = UTKFaceDataset(directory='../data/UTKFace_48', transform=transform)\n",
    "train_size = int(0.85 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-28T12:26:07.557191836Z",
     "start_time": "2024-01-28T12:26:07.556131908Z"
    }
   },
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество экземпляров каждого класса:\n",
      "Класс 0: 0 экземпляров\n",
      "Класс 1: 0 экземпляров\n",
      "Класс 2: 415 экземпляров\n",
      "Класс 3: 236 экземпляров\n",
      "Класс 4: 237 экземпляров\n",
      "Класс 5: 167 экземпляров\n",
      "Класс 6: 115 экземпляров\n",
      "Класс 7: 124 экземпляров\n",
      "Класс 8: 230 экземпляров\n",
      "Класс 9: 144 экземпляров\n",
      "Класс 10: 133 экземпляров\n",
      "Класс 11: 52 экземпляров\n",
      "Класс 12: 108 экземпляров\n",
      "Класс 13: 70 экземпляров\n",
      "Класс 14: 134 экземпляров\n",
      "Класс 15: 149 экземпляров\n",
      "Класс 16: 211 экземпляров\n",
      "Класс 17: 136 экземпляров\n",
      "Класс 18: 221 экземпляров\n",
      "Класс 19: 84 экземпляров\n",
      "Класс 20: 239 экземпляров\n",
      "Класс 21: 291 экземпляров\n",
      "Класс 22: 342 экземпляров\n",
      "Класс 23: 359 экземпляров\n",
      "Класс 24: 742 экземпляров\n",
      "Класс 25: 633 экземпляров\n",
      "Класс 26: 1863 экземпляров\n",
      "Класс 27: 518 экземпляров\n",
      "Класс 28: 773 экземпляров\n",
      "Класс 29: 478 экземпляров\n",
      "Класс 30: 626 экземпляров\n",
      "Класс 31: 302 экземпляров\n",
      "Класс 32: 556 экземпляров\n",
      "Класс 33: 125 экземпляров\n",
      "Класс 34: 350 экземпляров\n",
      "Класс 35: 754 экземпляров\n",
      "Класс 36: 403 экземпляров\n",
      "Класс 37: 250 экземпляров\n",
      "Класс 38: 273 экземпляров\n",
      "Класс 39: 221 экземпляров\n",
      "Класс 40: 441 экземпляров\n",
      "Класс 41: 109 экземпляров\n",
      "Класс 42: 227 экземпляров\n",
      "Класс 43: 131 экземпляров\n",
      "Класс 44: 90 экземпляров\n",
      "Класс 45: 375 экземпляров\n",
      "Класс 46: 138 экземпляров\n",
      "Класс 47: 141 экземпляров\n",
      "Класс 48: 131 экземпляров\n",
      "Класс 49: 121 экземпляров\n",
      "Класс 50: 319 экземпляров\n",
      "Класс 51: 125 экземпляров\n",
      "Класс 52: 195 экземпляров\n",
      "Класс 53: 201 экземпляров\n",
      "Класс 54: 294 экземпляров\n",
      "Класс 55: 231 экземпляров\n",
      "Класс 56: 207 экземпляров\n",
      "Класс 57: 87 экземпляров\n",
      "Класс 58: 227 экземпляров\n",
      "Класс 59: 64 экземпляров\n",
      "Класс 60: 252 экземпляров\n",
      "Класс 61: 132 экземпляров\n",
      "Класс 62: 113 экземпляров\n",
      "Класс 63: 81 экземпляров\n",
      "Класс 64: 44 экземпляров\n",
      "Класс 65: 218 экземпляров\n",
      "Класс 66: 66 экземпляров\n",
      "Класс 67: 77 экземпляров\n",
      "Класс 68: 86 экземпляров\n",
      "Класс 69: 47 экземпляров\n",
      "Класс 70: 124 экземпляров\n",
      "Класс 71: 25 экземпляров\n",
      "Класс 72: 85 экземпляров\n",
      "Класс 73: 53 экземпляров\n",
      "Класс 74: 30 экземпляров\n",
      "Класс 75: 122 экземпляров\n",
      "Класс 76: 52 экземпляров\n",
      "Класс 77: 26 экземпляров\n",
      "Класс 78: 53 экземпляров\n",
      "Класс 79: 20 экземпляров\n",
      "Класс 80: 112 экземпляров\n",
      "Класс 81: 18 экземпляров\n",
      "Класс 82: 35 экземпляров\n",
      "Класс 83: 16 экземпляров\n",
      "Класс 84: 18 экземпляров\n",
      "Класс 85: 135 экземпляров\n",
      "Класс 86: 31 экземпляров\n",
      "Класс 87: 9 экземпляров\n",
      "Класс 88: 23 экземпляров\n",
      "Класс 89: 30 экземпляров\n"
     ]
    }
   ],
   "source": [
    "# Использование встроенной функции для анализа классов\n",
    "class_counts = torch.zeros(90)\n",
    "\n",
    "for _, labels in train_loader:\n",
    "    class_counts += torch.bincount(labels, minlength=90)\n",
    "\n",
    "print(\"Количество экземпляров каждого класса:\")\n",
    "for i, count in enumerate(class_counts):\n",
    "    print(f\"Класс {i}: {int(count)} экземпляров\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-28T12:26:10.478224782Z",
     "start_time": "2024-01-28T12:26:07.556255259Z"
    }
   },
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "loss_fun = nn.MSELoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-28T12:26:11.000935231Z",
     "start_time": "2024-01-28T12:26:10.481443427Z"
    }
   },
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [24:49<00:00, 18.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(log_dir=LOG_PATH + \"/\" + datetime.now().strftime(TIME_FORMAT))\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS)):  # проход по датасету несколько раз\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = loss_fun(outputs.view(-1), labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    writer.add_scalar('Metrics/epoch_loss', running_loss  / len(train_loader), epoch)\n",
    "\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for i, (images, labels) in enumerate(test_loader):\n",
    "            images, labels = images.to(device), labels\n",
    "            outputs = model(images)\n",
    "            all_preds.extend(outputs.tolist())\n",
    "            all_labels.extend(labels.tolist())\n",
    "    err = mean_absolute_error(all_labels, all_preds)\n",
    "    writer.add_scalar('Metrics/MAE', err, epoch)\n",
    "\n",
    "print('Finished Training')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-28T12:51:00.564267721Z",
     "start_time": "2024-01-28T12:26:11.003423051Z"
    }
   },
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "torch.save(model, MODEL_PATH)\n",
    "torch.save(model.state_dict(), WEIGHTS_PATH)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-28T12:51:01.205184638Z",
     "start_time": "2024-01-28T12:51:00.563860968Z"
    }
   },
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Age: 32.08957290649414\n"
     ]
    }
   ],
   "source": [
    "# model = torch.load(MODEL_PATH)\n",
    "model.load_state_dict(torch.load(WEIGHTS_PATH))\n",
    "model.eval()\n",
    "\n",
    "image = Image.open(TEST_IMAGE_PATH)\n",
    "image = transform(image)\n",
    "image = image.to(device)\n",
    "image = image.unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(image)\n",
    "    predicted_age = output.item()\n",
    "\n",
    "print(f'Predicted Age: {predicted_age}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-28T16:20:39.800693918Z",
     "start_time": "2024-01-28T16:20:39.528783850Z"
    }
   },
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
