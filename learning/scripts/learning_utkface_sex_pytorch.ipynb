{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-08 23:44:02.288773: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-08 23:44:02.312659: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-08 23:44:02.312725: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-08 23:44:02.312757: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-08 23:44:02.318636: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import timm\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from datetime import datetime"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-08T20:44:03.809712299Z",
     "start_time": "2024-01-08T20:44:00.762318908Z"
    }
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "SIZE=48\n",
    "BATCH_SIZE=512"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-08T20:44:03.818624890Z",
     "start_time": "2024-01-08T20:44:03.809716206Z"
    }
   },
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class UTKFaceDataset(Dataset):\n",
    "    def __init__(self, directory, transform=None):\n",
    "        self.files = glob.glob(os.path.join(directory, '*.jpg'))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.files[idx]\n",
    "        image = Image.open(img_name)\n",
    "        filename = img_name.split('/')[-1]\n",
    "        sex = int(filename.split('_')[1])  # Предполагается, что имя файла начинается с возраста\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, sex"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-08T20:44:03.819733245Z",
     "start_time": "2024-01-08T20:44:03.813826098Z"
    }
   },
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((SIZE, SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "dataset = UTKFaceDataset(directory='../data/UTKFace_48', transform=transform)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-08T20:44:03.862374482Z",
     "start_time": "2024-01-08T20:44:03.818943973Z"
    }
   },
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class SexEstimatorModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SexEstimatorModel, self).__init__()\n",
    "        self.base_model = timm.create_model('efficientnetv2_rw_s', pretrained=True)\n",
    "        self.base_model.classifier = nn.Linear(self.base_model.classifier.in_features, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.base_model(x)\n",
    "        return torch.sigmoid(x).squeeze()  # Используйте сигмоиду для бинарной классификации\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-08T20:44:03.863088011Z",
     "start_time": "2024-01-08T20:44:03.862157692Z"
    }
   },
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SexEstimatorModel().to(device)\n",
    "loss_fun = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-08T20:44:05.007820566Z",
     "start_time": "2024-01-08T20:44:03.925360876Z"
    }
   },
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [23:39<00:00, 14.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter(log_dir=\"log/sex\", filename_suffix=datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "\n",
    "for epoch in tqdm(range(100)):  # проход по датасету несколько раз\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = loss_fun(outputs.view(-1), labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        writer.add_scalar('Metrics/epoch_loss', running_loss  / len(train_loader), epoch)\n",
    "    \n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for i, (images, labels) in enumerate(test_loader):\n",
    "            images, labels = images.to(device), labels\n",
    "            outputs = model(images)\n",
    "            preds = outputs.round()\n",
    "            all_preds.extend(preds.tolist())\n",
    "            all_labels.extend(labels.tolist())\n",
    "    precision = precision_score(all_labels, all_preds, average=\"weighted\")\n",
    "    recall = recall_score(all_labels, all_preds, average=\"weighted\")\n",
    "    f1 = f1_score(all_labels, all_preds, average=\"weighted\")\n",
    "    writer.add_scalar('Metrics/precision', precision, epoch)\n",
    "    writer.add_scalar('Metrics/recall', recall, epoch)\n",
    "    writer.add_scalar('Metrics/f1', f1, epoch)\n",
    "print('Finished Training')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-08T21:07:44.392112638Z",
     "start_time": "2024-01-08T20:44:05.012370388Z"
    }
   },
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "torch.save(model, '../../models/sex_model_torch.pth')\n",
    "torch.save(model.state_dict(), '../../models/sex_model_weights.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-08T21:07:44.938125175Z",
     "start_time": "2024-01-08T21:07:44.420406496Z"
    }
   },
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sex: 0.0\n"
     ]
    }
   ],
   "source": [
    "# model = SexEstimatorModel()  # Создайте экземпляр вашей модели\n",
    "# model.load_state_dict(torch.load('../../sex_model_torch.pth'))\n",
    "\n",
    "model = torch.load('../../models/sex_model_torch.pth')\n",
    "\n",
    "model.eval()  # Переведите модель в режим оценки\n",
    "\n",
    "# Загрузите изображение\n",
    "image_path = '../data/face_recognition_images/person1.1.jpg'\n",
    "image = Image.open(image_path)\n",
    "\n",
    "# Примените преобразования к изображению\n",
    "image = transform(image)\n",
    "image = image.to(device)\n",
    "image = image.unsqueeze(0)  # Добавьте дополнительное измерение, так как модель ожидает пакет изображений\n",
    "\n",
    "# Сделайте предсказание\n",
    "with torch.no_grad():\n",
    "    output = model(image)\n",
    "    predicted_sex = output.round().item()  # Получите предсказанный возраст как число\n",
    "\n",
    "print(f'Predicted Sex: {predicted_sex}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-08T21:11:03.674460031Z",
     "start_time": "2024-01-08T21:11:03.564020936Z"
    }
   },
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
